{
  "hash": "780063a3147497f8ec8f658bc6dbe0e9",
  "result": {
    "markdown": "---\ntitle: \"`{caret}` and beyond\"\nsubtitle: \"Statistical learning at scale\"\nauthor: Dmytro Perepolkin\ninstitute: Lund University\ntitle-slide-attributes:\n  data-background-image: \"img/carrot_33570.png\"\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nexecute: \n  echo: true\n  freeze: true\n  cache: true\nformat: \n  revealjs:\n    smaller: true\n    theme: blood\n---\n\n\n## History\n\n\n::: {.cell}\n\n:::\n\n\n{caret} (**C**lassification **A**nd **Re**gression **T**raining) created by Max Kuhn at Pfizer (Git: Jun-2005, CRAN: Oct-2007) primarily to simplify analysis of clinical trial data. \n\nMotivated by inconsistency in R package interfaces: formula interface, `x,y` interface or both? Default arguments?\n\n\n::: {#tbl-packages-syntax .cell tbl-cap='Interface for getting probability estimates from several classification models' hash='CaretAndBeyondSlides_cache/revealjs/tbl-packages-syntax_0b338e6ded137be63d3b0ebda3cce470'}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> obj </th>\n   <th style=\"text-align:left;\"> package </th>\n   <th style=\"text-align:left;\"> predict </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> lda </td>\n   <td style=\"text-align:left;\"> MASS </td>\n   <td style=\"text-align:left;\"> predict(obj) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> glm </td>\n   <td style=\"text-align:left;\"> stats </td>\n   <td style=\"text-align:left;\"> predict(obj, type='response') </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> gbm </td>\n   <td style=\"text-align:left;\"> gbm </td>\n   <td style=\"text-align:left;\"> predict(obj, type='response', n.trees) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mda </td>\n   <td style=\"text-align:left;\"> mda </td>\n   <td style=\"text-align:left;\"> predict(obj, type='posterior') </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> rpart </td>\n   <td style=\"text-align:left;\"> rpart </td>\n   <td style=\"text-align:left;\"> predict(obj, type='prob') </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Weka </td>\n   <td style=\"text-align:left;\"> RWeka </td>\n   <td style=\"text-align:left;\"> predict(obj, type='probability') </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> LogitBoost </td>\n   <td style=\"text-align:left;\"> caTools </td>\n   <td style=\"text-align:left;\"> predict(obj, type='raw', nIter) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## {background-image=\"img/E-P5vPtXEA0MBYS.jpeg\" background-size=contain}\n\n## Main features {page-layout=\"full\"}\n\n`{caret}` was conceived to:\n\n- unify *interfaces* to models (wrapper)\n- provide toolkit for *tuning* and resampling (CV)\n- pre-processing *helper* functions\n- one of the first *parallel processing* implementations\n\nSome visualization for EDA (based on `lattice`), unified interfaces for variable importance, measuring performance. Lots of feature selection methods.\n\nWe will be looking at some of the features using `Boston`\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-3_c92b60a067740e98c60d0d096e5e73cb'}\n\n```{.r .cell-code}\nboston <- MASS::Boston\n```\n:::\n\n\n## Models\n\nToday (2023-03-08) `{caret}` has wrappers for 239 models\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-4_199e8650d5ebc59a2a75902ac29100e2'}\n\n```{.r .cell-code}\noptions(width=120)\ngetModelInfo() %>% names()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] \"ada\"                 \"AdaBag\"              \"AdaBoost.M1\"         \"adaboost\"            \"amdai\"              \n  [6] \"ANFIS\"               \"avNNet\"              \"awnb\"                \"awtan\"               \"bag\"                \n [11] \"bagEarth\"            \"bagEarthGCV\"         \"bagFDA\"              \"bagFDAGCV\"           \"bam\"                \n [16] \"bartMachine\"         \"bayesglm\"            \"binda\"               \"blackboost\"          \"blasso\"             \n [21] \"blassoAveraged\"      \"bridge\"              \"brnn\"                \"BstLm\"               \"bstSm\"              \n [26] \"bstTree\"             \"C5.0\"                \"C5.0Cost\"            \"C5.0Rules\"           \"C5.0Tree\"           \n [31] \"cforest\"             \"chaid\"               \"CSimca\"              \"ctree\"               \"ctree2\"             \n [36] \"cubist\"              \"dda\"                 \"deepboost\"           \"DENFIS\"              \"dnn\"                \n [41] \"dwdLinear\"           \"dwdPoly\"             \"dwdRadial\"           \"earth\"               \"elm\"                \n [46] \"enet\"                \"evtree\"              \"extraTrees\"          \"fda\"                 \"FH.GBML\"            \n [51] \"FIR.DM\"              \"foba\"                \"FRBCS.CHI\"           \"FRBCS.W\"             \"FS.HGD\"             \n [56] \"gam\"                 \"gamboost\"            \"gamLoess\"            \"gamSpline\"           \"gaussprLinear\"      \n [61] \"gaussprPoly\"         \"gaussprRadial\"       \"gbm_h2o\"             \"gbm\"                 \"gcvEarth\"           \n [66] \"GFS.FR.MOGUL\"        \"GFS.LT.RS\"           \"GFS.THRIFT\"          \"glm.nb\"              \"glm\"                \n [71] \"glmboost\"            \"glmnet_h2o\"          \"glmnet\"              \"glmStepAIC\"          \"gpls\"               \n [76] \"hda\"                 \"hdda\"                \"hdrda\"               \"HYFIS\"               \"icr\"                \n [81] \"J48\"                 \"JRip\"                \"kernelpls\"           \"kknn\"                \"knn\"                \n [86] \"krlsPoly\"            \"krlsRadial\"          \"lars\"                \"lars2\"               \"lasso\"              \n [91] \"lda\"                 \"lda2\"                \"leapBackward\"        \"leapForward\"         \"leapSeq\"            \n [96] \"Linda\"               \"lm\"                  \"lmStepAIC\"           \"LMT\"                 \"loclda\"             \n[101] \"logicBag\"            \"LogitBoost\"          \"logreg\"              \"lssvmLinear\"         \"lssvmPoly\"          \n[106] \"lssvmRadial\"         \"lvq\"                 \"M5\"                  \"M5Rules\"             \"manb\"               \n[111] \"mda\"                 \"Mlda\"                \"mlp\"                 \"mlpKerasDecay\"       \"mlpKerasDecayCost\"  \n[116] \"mlpKerasDropout\"     \"mlpKerasDropoutCost\" \"mlpML\"               \"mlpSGD\"              \"mlpWeightDecay\"     \n[121] \"mlpWeightDecayML\"    \"monmlp\"              \"msaenet\"             \"multinom\"            \"mxnet\"              \n[126] \"mxnetAdam\"           \"naive_bayes\"         \"nb\"                  \"nbDiscrete\"          \"nbSearch\"           \n[131] \"neuralnet\"           \"nnet\"                \"nnls\"                \"nodeHarvest\"         \"null\"               \n[136] \"OneR\"                \"ordinalNet\"          \"ordinalRF\"           \"ORFlog\"              \"ORFpls\"             \n[141] \"ORFridge\"            \"ORFsvm\"              \"ownn\"                \"pam\"                 \"parRF\"              \n[146] \"PART\"                \"partDSA\"             \"pcaNNet\"             \"pcr\"                 \"pda\"                \n[151] \"pda2\"                \"penalized\"           \"PenalizedLDA\"        \"plr\"                 \"pls\"                \n[156] \"plsRglm\"             \"polr\"                \"ppr\"                 \"pre\"                 \"PRIM\"               \n[161] \"protoclass\"          \"qda\"                 \"QdaCov\"              \"qrf\"                 \"qrnn\"               \n[166] \"randomGLM\"           \"ranger\"              \"rbf\"                 \"rbfDDA\"              \"Rborist\"            \n[171] \"rda\"                 \"regLogistic\"         \"relaxo\"              \"rf\"                  \"rFerns\"             \n[176] \"RFlda\"               \"rfRules\"             \"ridge\"               \"rlda\"                \"rlm\"                \n[181] \"rmda\"                \"rocc\"                \"rotationForest\"      \"rotationForestCp\"    \"rpart\"              \n[186] \"rpart1SE\"            \"rpart2\"              \"rpartCost\"           \"rpartScore\"          \"rqlasso\"            \n[191] \"rqnc\"                \"RRF\"                 \"RRFglobal\"           \"rrlda\"               \"RSimca\"             \n[196] \"rvmLinear\"           \"rvmPoly\"             \"rvmRadial\"           \"SBC\"                 \"sda\"                \n[201] \"sdwd\"                \"simpls\"              \"SLAVE\"               \"slda\"                \"smda\"               \n[206] \"snn\"                 \"sparseLDA\"           \"spikeslab\"           \"spls\"                \"stepLDA\"            \n[211] \"stepQDA\"             \"superpc\"             \"svmBoundrangeString\" \"svmExpoString\"       \"svmLinear\"          \n[216] \"svmLinear2\"          \"svmLinear3\"          \"svmLinearWeights\"    \"svmLinearWeights2\"   \"svmPoly\"            \n[221] \"svmRadial\"           \"svmRadialCost\"       \"svmRadialSigma\"      \"svmRadialWeights\"    \"svmSpectrumString\"  \n[226] \"tan\"                 \"tanSearch\"           \"treebag\"             \"vbmpRadial\"          \"vglmAdjCat\"         \n[231] \"vglmContRatio\"       \"vglmCumulative\"      \"widekernelpls\"       \"WM\"                  \"wsrf\"               \n[236] \"xgbDART\"             \"xgbLinear\"           \"xgbTree\"             \"xyf\"                \n```\n:::\n:::\n\n\n## Getting started\n\n### EDA\n\n\"Index\"-returning functions to subset columns.\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-5_403d56d956dd3b476ea1dd129b47e45b'}\n\n```{.r .cell-code}\nnzv <- nearZeroVar(boston)\nif (length(nzv)>0) boston <- boston[,-nzv]\n\nhco <- findCorrelation(cor(boston), cutoff = .9)\nif (length(hco)>0) boston <- boston[,-hco]\n\nlco <- findLinearCombos(boston)\nif (length(lco$remove)>0) boston <- boston[,-lco$remove]\n```\n:::\n\n\n### Partitioning\n\nCreates *stratified* split (balanced classes/quantiles across partitions). \n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-6_e44dd3f818d9471dd9bf01b5a789096f'}\n\n```{.r .cell-code}\nset.seed(42)\ntrain_idx <- createDataPartition(boston$medv, p=0.7, list = FALSE)\nboston_train <- boston[train_idx,]\nboston_test <- boston[-train_idx,]\n```\n:::\n\n\nAlso: `createFolds`, `createMultiFolds`, `createResamples`, `createTimeSlices`\n\n## Preprocessing\n\nPre-processing creates a *recipe* for data manipulation, but does not apply the operations (until you call `predict()`). \n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-7_e66ada3d753ddf763bc70131c387c6c2'}\n\n```{.r .cell-code}\npreproc_steps <- preProcess(boston_train, method=c(\"scale\", \"center\"))\npreproc_steps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCreated from 356 samples and 13 variables\n\nPre-processing:\n  - centered (13)\n  - ignored (0)\n  - scaled (13)\n```\n:::\n\n```{.r .cell-code}\nboston_proc_train <- predict(preproc_steps, boston_train)\nboston_proc_test <- predict(preproc_steps, boston_test)\n```\n:::\n\n\nCan also perform imputation (e.g. `knnImpute`, `bagImpute`), as well as `pca`/`ica`, `nzv` and non-linear transforms `BoxCox`, `YeoJohnson`, etc.\n\nWhy you should not `preProcess(boston, ...)`?\n\n## Train {background-image=\"img/mireo-plus-b.png\" background-size=\"200px\" background-position=\"top 10px right 10px\"}\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-8_76259e28a371b2c20d77dd48c4038f70'}\n\n```{.r .cell-code}\nset.seed(42)\ntc <- trainControl(method = \"repeatedcv\", \n                   number = 10, repeats = 10)\ngbm_fit <- train(medv ~ ., data = boston_proc_train, \n                 method = \"gbm\", trControl = tc, verbose=FALSE)\ngbm_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStochastic Gradient Boosting \n\n356 samples\n 12 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 320, 321, 322, 320, 321, 320, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  RMSE       Rsquared   MAE      \n  1                   50      0.4539947  0.7898859  0.3140605\n  1                  100      0.4253064  0.8106993  0.2935521\n  1                  150      0.4166116  0.8189453  0.2885573\n  2                   50      0.4123146  0.8230713  0.2844296\n  2                  100      0.3895082  0.8409736  0.2706038\n  2                  150      0.3803161  0.8490129  0.2663978\n  3                   50      0.3950314  0.8369821  0.2715722\n  3                  100      0.3726304  0.8545770  0.2587302\n  3                  150      0.3636345  0.8622549  0.2554157\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\nTuning parameter 'n.minobsinnode' was held constant\n at a value of 10\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 150, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.\n```\n:::\n:::\n\n\n`trainControl()` is the main workhorse for cross-validation (and tuning). It creates grids for you and performs adaptive search to minimize the computational cost.\n\n## Tuning\n\nYou can manually perform the tuning of hyperparameters overriding the default grids `trainControl()` creates for you.\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-9_a4451860b5eed6a86c69bed316d4c078'}\n\n```{.r .cell-code  code-line-numbers=\"1-4,8\"}\ngbm_grid <-  expand.grid(interaction.depth = c(1, 5, 9), \n                        n.trees = (1:3)*50, \n                        shrinkage = c(0.1, 0.2, 0.3),\n                        n.minobsinnode = c(10,20))\nnrow(gbm_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 54\n```\n:::\n\n```{.r .cell-code  code-line-numbers=\"1-4,8\"}\ngbm_fit_tuned <- train(medv ~ ., data = boston_proc_train, \n                 method = \"gbm\", trControl = tc, verbose=FALSE, \n                 tuneGrid=gbm_grid)\ngbm_fit_tuned\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStochastic Gradient Boosting \n\n356 samples\n 12 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 320, 320, 320, 320, 321, 321, ... \nResampling results across tuning parameters:\n\n  shrinkage  interaction.depth  n.minobsinnode  n.trees  RMSE       Rsquared   MAE      \n  0.1        1                  10               50      0.4570190  0.7920241  0.3170863\n  0.1        1                  10              100      0.4292733  0.8130271  0.2952967\n  0.1        1                  10              150      0.4185138  0.8221516  0.2896760\n  0.1        1                  20               50      0.4801508  0.7710848  0.3296117\n  0.1        1                  20              100      0.4552362  0.7901258  0.3126133\n  0.1        1                  20              150      0.4475288  0.7971568  0.3085887\n  0.1        5                  10               50      0.3774114  0.8558603  0.2598620\n  0.1        5                  10              100      0.3586280  0.8698706  0.2488435\n  0.1        5                  10              150      0.3530236  0.8743001  0.2471894\n  0.1        5                  20               50      0.4291829  0.8137281  0.2914289\n  0.1        5                  20              100      0.4055895  0.8330371  0.2801370\n  0.1        5                  20              150      0.3974958  0.8399539  0.2780558\n  0.1        9                  10               50      0.3703353  0.8596494  0.2499267\n  0.1        9                  10              100      0.3554966  0.8708583  0.2446527\n  0.1        9                  10              150      0.3499599  0.8750702  0.2444459\n  0.1        9                  20               50      0.4254119  0.8170942  0.2890846\n  0.1        9                  20              100      0.4079924  0.8311823  0.2812808\n  0.1        9                  20              150      0.3988880  0.8389970  0.2783244\n  0.2        1                  10               50      0.4332482  0.8101956  0.3001468\n  0.2        1                  10              100      0.4191745  0.8229897  0.2944498\n  0.2        1                  10              150      0.4146920  0.8269669  0.2944515\n  0.2        1                  20               50      0.4624986  0.7844358  0.3182666\n  0.2        1                  20              100      0.4528254  0.7936661  0.3145203\n  0.2        1                  20              150      0.4471099  0.7991055  0.3137016\n  0.2        5                  10               50      0.3725533  0.8596403  0.2603050\n  0.2        5                  10              100      0.3658036  0.8660857  0.2597914\n  0.2        5                  10              150      0.3672296  0.8656500  0.2624443\n  0.2        5                  20               50      0.4166088  0.8249074  0.2901481\n  0.2        5                  20              100      0.4057305  0.8352627  0.2871671\n  0.2        5                  20              150      0.4032890  0.8377661  0.2885071\n  0.2        9                  10               50      0.3687060  0.8629401  0.2566159\n  0.2        9                  10              100      0.3666536  0.8650538  0.2601301\n  0.2        9                  10              150      0.3669103  0.8652173  0.2621234\n  0.2        9                  20               50      0.4110325  0.8288678  0.2894632\n  0.2        9                  20              100      0.3994266  0.8397191  0.2870193\n  0.2        9                  20              150      0.3984494  0.8413134  0.2883171\n  0.3        1                  10               50      0.4265918  0.8162571  0.3001828\n  0.3        1                  10              100      0.4180720  0.8246033  0.2977236\n  0.3        1                  10              150      0.4164494  0.8259661  0.2993284\n  0.3        1                  20               50      0.4582224  0.7889827  0.3195430\n  0.3        1                  20              100      0.4491875  0.7978232  0.3179120\n  0.3        1                  20              150      0.4453258  0.8013104  0.3181259\n  0.3        5                  10               50      0.3832109  0.8532121  0.2744719\n  0.3        5                  10              100      0.3810754  0.8554531  0.2756060\n  0.3        5                  10              150      0.3833570  0.8540534  0.2783372\n  0.3        5                  20               50      0.4224557  0.8203428  0.3001696\n  0.3        5                  20              100      0.4164047  0.8267242  0.3020547\n  0.3        5                  20              150      0.4156130  0.8287723  0.3030371\n  0.3        9                  10               50      0.3832002  0.8518751  0.2726441\n  0.3        9                  10              100      0.3844557  0.8524790  0.2771106\n  0.3        9                  10              150      0.3865055  0.8512256  0.2792891\n  0.3        9                  20               50      0.4209287  0.8221709  0.3030387\n  0.3        9                  20              100      0.4141053  0.8297027  0.3027560\n  0.3        9                  20              150      0.4166066  0.8286794  0.3049680\n\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 150, interaction.depth = 9, shrinkage = 0.1 and n.minobsinnode = 10.\n```\n:::\n:::\n\n## Visualization\n\nMost objects are visualizable.\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-10_88b241733749f085ed43857d66bd1b53'}\n\n```{.r .cell-code}\nplot(gbm_fit_tuned)\n```\n\n::: {.cell-output-display}\n![](CaretAndBeyondSlides_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n## Visualization\n\nAlso in `ggplot`\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-11_cc6bf9e37240a3b3005d31791334fedd'}\n\n```{.r .cell-code}\nggplot(gbm_fit_tuned)\n```\n\n::: {.cell-output-display}\n![](CaretAndBeyondSlides_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n## Occam's razor\n\nTuning produces many alternative models. The best model is probably on plateau, and we could try to find a simpler model which is \"about as good\". This will find the simplest model within 3 pct of the best one.\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-12_8a47a2623f83934e1bf680de2ccb78bc'}\n\n```{.r .cell-code}\nalso_good_mod <- tolerance(gbm_fit_tuned$results, metric=\"RMSE\",\n                       tol=3, maximize = FALSE)\ngbm_fit_tuned$results[which.min(gbm_fit_tuned$results$RMSE),]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   shrinkage interaction.depth n.minobsinnode n.trees      RMSE  Rsquared\n15       0.1                 9             10     150 0.3499599 0.8750702\n         MAE     RMSESD RsquaredSD      MAESD\n15 0.2444459 0.08563751 0.05817571 0.04037827\n```\n:::\n\n```{.r .cell-code}\ngbm_fit_tuned$results[also_good_mod,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  shrinkage interaction.depth n.minobsinnode n.trees     RMSE  Rsquared\n8       0.1                 5             10     100 0.358628 0.8698706\n        MAE     RMSESD RsquaredSD     MAESD\n8 0.2488435 0.08910584 0.06325679 0.0422828\n```\n:::\n:::\n\n\nYou can, of course, identify this visually (using some sort of elbow plot).\n\n## Multiple models\n\nSwitching models has never been easier\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-13_02a18a6b91c0f66d6e2b282b86386bec'}\n\n```{.r .cell-code}\nsvm_fit <- train(medv ~ ., data = boston_proc_train, \n                 method = \"svmRadial\", trControl = tc, verbose=FALSE)\nrf_fit <- train(medv ~ ., data = boston_proc_train, \n                 method = \"rf\", trControl = tc, verbose=FALSE)\n```\n:::\n\n\nEach algorithm would require a customized grid, but thankfully, `trainControl()` will do some tuning for you (3 values for each tunable parameter).\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-14_b79bfd9ba38a53722eec9588046e898b'}\n\n```{.r .cell-code}\nres <- resamples(list(GBM = gbm_fit,\n                      SVM = svm_fit,\n                      RF = rf_fit))\nsummary(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsummary.resamples(object = res)\n\nModels: GBM, SVM, RF \nNumber of resamples: 100 \n\nMAE \n         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nGBM 0.1596753 0.2294928 0.2536258 0.2554157 0.2772792 0.3823098    0\nSVM 0.1941852 0.2471878 0.2780476 0.2825368 0.3139017 0.4532184    0\nRF  0.1469884 0.2188436 0.2428722 0.2454496 0.2691248 0.4388493    0\n\nRMSE \n         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nGBM 0.2119712 0.3052745 0.3463568 0.3636345 0.4033769 0.6899072    0\nSVM 0.2501915 0.3580981 0.4175420 0.4354080 0.4767103 0.7894043    0\nRF  0.1943300 0.2998365 0.3380906 0.3539211 0.3990887 0.8044251    0\n\nRsquared \n         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nGBM 0.5919561 0.8278442 0.8824322 0.8622549 0.9071705 0.9667030    0\nSVM 0.4703106 0.7850482 0.8426367 0.8156808 0.8807625 0.9344111    0\nRF  0.5950725 0.8605203 0.8870178 0.8792928 0.9068011 0.9690597    0\n```\n:::\n:::\n\n\n## Variable importance\n\nVariable importance mean different things for different algorithms. `caret` has unified interface for variable importance\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-15_4e6be5c7644d1cff9f16db92c8cd26ec'}\n\n```{.r .cell-code}\nvarImp(rf_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrf variable importance\n\n          Overall\nrm      100.00000\nlstat    76.23957\nnox      14.04357\ndis      13.98319\ncrim     12.55759\nindus     7.54353\nptratio   7.07155\nage       3.65865\nblack     2.38567\nrad       0.54808\nchas      0.02715\nzn        0.00000\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-16_8cdb766457395f3fe8e16a8f9da7feb7'}\n\n```{.r .cell-code}\nvarImp(svm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nloess r-squared variable importance\n\n        Overall\nlstat    100.00\nrm        90.44\nindus     31.25\nptratio   30.87\nnox       27.71\ncrim      27.31\nblack     21.92\nage       17.36\nrad       16.87\nzn        14.52\ndis       11.59\nchas       0.00\n```\n:::\n:::\n\n:::\n\n::::\n\n## Prediction\n\nPrediction is also easy. Select a model and call\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-17_7282792d4feb38bc5bbd60b6dacb5b7f'}\n\n```{.r .cell-code}\nrf_preds <- predict(rf_fit, newdata=boston_proc_test)\nsvm_preds <- predict(svm_fit, newdata=boston_proc_test)\n```\n:::\n\n\nPerformance measurement varies between regression and classification. Wide selection of custom metrics are available (and you can add your own).\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-18_8672ecb9f18fef1bf54842f383a0ea69'}\n\n```{.r .cell-code}\npostResample(pred = rf_preds, obs = boston_proc_test$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     RMSE  Rsquared       MAE \n0.3316805 0.8816282 0.2274026 \n```\n:::\n\n```{.r .cell-code}\npostResample(pred = svm_preds, obs = boston_proc_test$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     RMSE  Rsquared       MAE \n0.3799208 0.8437500 0.2277842 \n```\n:::\n:::\n\n\n## Classification example\n\nLet's predict whether the house is located on Charles river. \n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-19_39e78bdb16ce059e4c104b5c294deb34'}\n\n```{.r .cell-code}\nset.seed(42)\nboston$chas <- factor(boston$chas, levels=c(0,1), labels=c(\"NotRiverside\", \"Riverside\"))\nctrain_idx <- createDataPartition(boston$chas, p=0.7, list = FALSE)\nboston_ctrain <- boston[ctrain_idx,]\nboston_ctest <- boston[-ctrain_idx,]\n```\n:::\n\n\nWe need to do something about class imbalance. \n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-20_ce2404fbcae5079417164d5a8f20180f'}\n\n```{.r .cell-code}\ntable(boston$chas) %>% prop.table()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nNotRiverside    Riverside \n  0.93083004   0.06916996 \n```\n:::\n:::\n\n\nThis means that we can expect our classifier to be at least this good by predicting no riverside houses.\n\n:::{.callout-caution}\nClass imbalance is one of the first thing you want to look at because it will influence how you want to look at the results and how you want to preprocess your data.\n:::\n\n## Train with preprocessing\n\nWe create a trainControl which is slightly customized for classification\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-21_c08802e482177ab26c53b83177fa0b43'}\n\n```{.r .cell-code}\nset.seed(42)\nctr <- trainControl(method=\"repeatedcv\", repeats=5,\n                    summaryFunction = twoClassSummary,\n                    classProbs = TRUE)\ncfit_rf <- train(chas ~ ., data = boston_ctrain, \n                preProcess=c(\"scale\", \"center\"), metric=\"ROC\",\n                method = \"rf\", trControl = ctr, verbose=FALSE)\n```\n:::\n\n\nWe can downsample the dominant class (non-riverside housing) and refit the model (setting `sampling=\"down\"`) or upsample the minority class (`sampling=\"up\"`) or do some combination of the two (`smote` or `rose`). But at the end of the day the problem is unlikely to go away. \n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-22_10afb02d5b43baf422602ca9c928d2be'}\n\n```{.r .cell-code}\nctr$sampling <- \"down\"\ncfit_rf_down <- train(chas ~ ., data = boston_ctrain, \n                preProcess=c(\"scale\", \"center\"), metric=\"ROC\",\n                method = \"rf\", trControl = ctr, verbose=FALSE)\nctr$sampling <- \"rose\"\ncfit_rf_rose <- train(chas ~ ., data = boston_ctrain, \n                preProcess=c(\"scale\", \"center\"), metric=\"ROC\",\n                method = \"rf\", trControl = ctr, verbose=FALSE)\n```\n:::\n\n\n## Class imbalance\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-23_349b4c2a20bcf6dd0d26f49b491859ed'}\n\n```{.r .cell-code}\ncfit_rf_models <- resamples(list(\n  original=cfit_rf,\n  down=cfit_rf_down,\n  rose=cfit_rf_rose))\nsummary(cfit_rf_models)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsummary.resamples(object = cfit_rf_models)\n\nModels: original, down, rose \nNumber of resamples: 50 \n\nROC \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\noriginal 0.5909091 0.8371212 0.9141414 0.8817172 0.9393939 1.0000000    0\ndown     0.3409091 0.7171717 0.8207071 0.7900000 0.8787879 0.9545455    0\nrose     0.1818182 0.5833333 0.7222222 0.7013131 0.8030303 1.0000000    0\n\nSens \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\noriginal 0.8787879 0.9696970 0.9696970 0.9769697 1.0000000 1.0000000    0\ndown     0.4242424 0.6060606 0.6666667 0.6787879 0.7575758 0.8787879    0\nrose     0.3030303 0.4242424 0.4848485 0.4963636 0.5757576 0.7272727    0\n\nSpec \n         Min.   1st Qu. Median       Mean 3rd Qu. Max. NA's\noriginal    0 0.0000000      0 0.07666667       0  0.5    0\ndown        0 0.6666667      1 0.79333333       1  1.0    0\nrose        0 0.6666667      1 0.81000000       1  1.0    0\n```\n:::\n:::\n\n\n## Class imbalance\n\nDoes it even mean anything?\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-24_f0b2fc399c32c24f0e165d51bfb2c2d8'}\n\n```{.r .cell-code}\npreds_rf <- predict(cfit_rf, newdata=boston_ctest)\nconfusionMatrix(data=preds_rf, reference=boston_ctest$chas)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n              Reference\nPrediction     NotRiverside Riverside\n  NotRiverside          138         7\n  Riverside               3         3\n                                          \n               Accuracy : 0.9338          \n                 95% CI : (0.8816, 0.9678)\n    No Information Rate : 0.9338          \n    P-Value [Acc > NIR] : 0.5831          \n                                          \n                  Kappa : 0.3423          \n                                          \n Mcnemar's Test P-Value : 0.3428          \n                                          \n            Sensitivity : 0.9787          \n            Specificity : 0.3000          \n         Pos Pred Value : 0.9517          \n         Neg Pred Value : 0.5000          \n             Prevalence : 0.9338          \n         Detection Rate : 0.9139          \n   Detection Prevalence : 0.9603          \n      Balanced Accuracy : 0.6394          \n                                          \n       'Positive' Class : NotRiverside    \n                                          \n```\n:::\n:::\n\n\n## Class imbalance\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-25_35c08eb204c179afb7102a277fdf25ef'}\n\n```{.r .cell-code}\nprobs_rf <- predict(cfit_rf, newdata=boston_ctest, type=\"prob\")\npROC::roc(boston_ctest$chas,probs_rf$Riverside, levels=levels(boston_ctest$chas)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nroc.default(response = boston_ctest$chas, predictor = probs_rf$Riverside,     levels = levels(boston_ctest$chas))\n\nData: probs_rf$Riverside in 141 controls (boston_ctest$chas NotRiverside) < 10 cases (boston_ctest$chas Riverside).\nArea under the curve: 0.9053\n```\n:::\n\n```{.r .cell-code}\nprobs_rf_down <- predict(cfit_rf_down, newdata=boston_ctest, type=\"prob\")\npROC::roc(boston_ctest$chas,probs_rf_down$Riverside, levels=levels(boston_ctest$chas))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nroc.default(response = boston_ctest$chas, predictor = probs_rf_down$Riverside,     levels = levels(boston_ctest$chas))\n\nData: probs_rf_down$Riverside in 141 controls (boston_ctest$chas NotRiverside) < 10 cases (boston_ctest$chas Riverside).\nArea under the curve: 0.8181\n```\n:::\n\n```{.r .cell-code}\nprobs_rf_rose <- predict(cfit_rf_rose, newdata=boston_ctest, type=\"prob\")\npROC::roc(boston_ctest$chas,probs_rf_rose$Riverside, levels=levels(boston_ctest$chas))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nroc.default(response = boston_ctest$chas, predictor = probs_rf_rose$Riverside,     levels = levels(boston_ctest$chas))\n\nData: probs_rf_rose$Riverside in 141 controls (boston_ctest$chas NotRiverside) < 10 cases (boston_ctest$chas Riverside).\nArea under the curve: 0.833\n```\n:::\n:::\n\n\n## Class imbalance\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-26_d5f45a2abf686ff07cdf57d23f95ba55'}\n\n```{.r .cell-code}\nrf_roc <- pROC::roc(boston_ctest$chas,probs_rf$Riverside, levels=levels(boston_ctest$chas))\nbest_th <- pROC::coords(rf_roc, \"best\")$threshold\nplot(rf_roc, \n     print.thres=c(0.5, best_th))\n```\n\n::: {.cell-output-display}\n![](CaretAndBeyondSlides_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n## Class imbalance\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-27_f2b5a4c2b723b1a763ce627926c8f7bc'}\n\n```{.r .cell-code}\nrf_roc_rose <- pROC::roc(boston_ctest$chas,probs_rf_rose$Riverside, levels=levels(boston_ctest$chas))\nlist(\"RF\"=rf_roc, \"ROSE\"=rf_roc_rose) %>% pROC::ggroc()\n```\n\n::: {.cell-output-display}\n![](CaretAndBeyondSlides_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n## More models\n\nA ton of available models. Documentation at `{caret}` [website](https://topepo.github.io/caret/available-models.html). Models are organized by task and clustered by tag.\n\nSwitching the model is as easy as swapping `method=\"__\"` in `train()` (unless you do manual grid search).  A lot can be done with model combination (stacking), but you want to be careful with folding to avoid leakage. `{caret}` allows you to craft the CV indices manually.\n\n### Way forward\n\n- Multiple models has been first introduced in R4DS (\"R for Data Science\") book. Neat idea based on list-columns and extensive use of `purrr::map()` to iterate over them. `{modelr}` package by Hadley. \n- Max Kuhn was leaving Pfizer and the future of `caret` was dim. Rstudio snapped him to continue working on streamlining modeling interfaces in R.\n\n## Beyond {caret}\n\n- First project was `rsample` which created a new type of data structure for repeated sampling - a list-column of indices (sort of unevaluated pointers to future data). \"Conscious decoupling\". Caret is too big.\n- Then came \n   - `recipes` - further development of unevaluated promises for pre-processing\n   - `parsnip` - rethinking of `caret` for tidyverse\n   - `dials` - tuning functions\n   - `yardstick` - helper functions for measuring performance\n   - `baguette` - generalized framework for bagging\n   - `stacks` - generalized framework for stacking\n   - `workflow` - ML workflow automation (inspired by sci-kit learn).\n\nMore details in the new book [Tidy Modeling with R](https://www.tmwr.org/) by Max Kuhn and Julia Silge\n\n## Tidymodels\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-28_6354ad312b315415810383f6714a3b0e'}\n\n```{.r .cell-code}\nlibrary(tidymodels) #meta-package\n\nset.seed(42)\nbh_split <- initial_split(boston, prop=0.7, strata=\"medv\")\nbh_recipe <- training(bh_split) %>% \n  recipe(medv~.) %>% \n  step_dummy(chas) %>% \n  step_corr(all_predictors()) %>% \n  step_nzv(all_predictors())\nbh_recipe %>% prep()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Inputs \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNumber of variables by role\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\noutcome:    1\npredictor: 12\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Training information \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nTraining data contained 352 data points and no incomplete rows.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Operations \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n• Dummy variables from: chas | Trained\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n• Correlation filter on: <none> | Trained\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n• Sparse, unbalanced variable filter removed: <none> | Trained\n```\n:::\n:::\n\n## Tidymodels\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-29_4bd9538eada975b61190a86b594ef4ec'}\n\n```{.r .cell-code}\nbh_lm <- linear_reg(mode = \"regression\") %>%\n  set_engine(\"lm\")\n\nbh_lm_wf <- workflow() %>%\n  add_recipe(bh_recipe) %>%\n  add_model(bh_lm)\n\nbh_rt <- decision_tree(mode = \"regression\") %>%\n  set_engine(\"rpart\")\n\nbh_rt_wf <- workflow() %>%\n  add_recipe(bh_recipe) %>%\n  add_model(bh_rt)\n\nset.seed(42)\nbh_folds <- vfold_cv(training(bh_split), strata = \"medv\", v = 3, repeats = 3)\nmetrics_regression <- metric_set(rmse, mae, rsq)\n```\n:::\n\n\n## Tidymodels\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-30_9a94505b97d9b27cbaf626a684278843'}\n\n```{.r .cell-code}\nset.seed(42)\nlm_fit <- fit_resamples(bh_lm_wf, bh_folds, metrics = metrics_regression) %>%\n  collect_metrics() %>%\n  mutate(model = \"lm\")\n\nset.seed(42)\nrt_fit <- fit_resamples(bh_rt_wf, bh_folds, metrics = metrics_regression) %>%\n  collect_metrics() %>%\n  mutate(model = \"rt\")\n```\n:::\n\n::: {.cell output-location='column' hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-31_5c1994f5d554cf9aef4d7bad3c17cfb2'}\n\n```{.r .cell-code}\nbind_rows(lm_fit, rt_fit) %>%\n  select(.metric, mean, std_err, model) %>%\n  ggplot(aes(x = model, y = mean, \n             ymin = mean - 1.96*std_err, \n             ymax = mean + 1.96*std_err)) +\n  geom_pointrange() + \n  labs(y = \"confidence interval\") +\n  facet_grid(. ~ .metric)\n```\n\n::: {.cell-output-display}\n![](CaretAndBeyondSlides_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n\n## Tidymodels\n\n\n::: {.cell hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-32_ecc65cbf3a9bb5b48487642395acfc7c'}\n\n```{.r .cell-code}\nfitted_model <- bh_lm_wf %>%\n  fit(training(bh_split))\npredict_test <- fitted_model %>%\n  predict(testing(bh_split)) %>%\n  bind_cols(testing(bh_split)) \n\npredict_test %>%\n  metrics_regression(truth = medv, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       4.46 \n2 mae     standard       3.13 \n3 rsq     standard       0.742\n```\n:::\n:::\n\n::: {.cell output-location='column' hash='CaretAndBeyondSlides_cache/revealjs/unnamed-chunk-33_fecae4aaa3b9ff921b947171afd7f84e'}\n\n```{.r .cell-code}\nbind_rows(predict_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, \n        size = 0.3, linetype = \"dashed\") \n```\n\n::: {.cell-output-display}\n![](CaretAndBeyondSlides_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n:::\n\n\n# Thank you!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"CaretAndBeyondSlides_files/libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"CaretAndBeyondSlides_files/libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}